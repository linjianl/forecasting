{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "import logging\n",
    "import importlib\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql import functions as f, types as t\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "NUM_PARTITIONS = 1024\n",
    "spark = ( SparkSession\n",
    "             .builder\n",
    "             .config(\"spark.app.name\",\"prophetPredictionConfig\")\n",
    "             .config(\"spark.yarn.queue\",\"root.sp_analytics.spark\")\n",
    "             .config(\"spark.default.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n",
    "             .config(\"spark.speculation\",\"true\")\n",
    "             .config(\"spark.default.parallelism\",str(NUM_PARTITIONS))\n",
    "             .config(\"spark.sql.shuffle.partitions\",str(NUM_PARTITIONS))\n",
    "             .config(\"spark.yarn.executor.memoryOverhead\", str(8192))\n",
    "             .enableHiveSupport()\n",
    "             .getOrCreate() )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "account = \"trivago\"\n",
    "run_date =str(datetime.today().date())\n",
    "optimizer=\"random_search\"\n",
    "forecast_horizon=90\n",
    "metric_class='GrossBookings'\n",
    "eval_level = 'daily'\n",
    "pos = 'All'\n",
    "has_add_regressors = False\n",
    "cv_type = \"rolling_window\"\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = run_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,format=\"%(asctime)s %(levelname)-10s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 14:37:39,634 INFO       current_dir is /home/lijia/git_tree/lin_jia/forecasting/daily_metrics/\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir = current_dir[:current_dir.find('scripts')]\n",
    "logging.info(\"current_dir is {0}\".format(current_dir))\n",
    "mod_file = os.path.join(current_dir, \"forecast_lib/utils/zip_module.py\")\n",
    "sc.addPyFile(mod_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_zipping = importlib.import_module(\"zip_module\")\n",
    "mod_zipping.zip_and_ship_module(current_dir)\n",
    "current_dir = os.getcwd()\n",
    "zipped_file = os.path.join(current_dir,\"forecast_lib.zip\")\n",
    "sc.addPyFile(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = importlib.import_module(\"forecast_lib.account.{0}\".format(account))\n",
    "Metrics = getattr(mod, metric_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from forecast_lib.utils.prepare_df_to_prophet import prepare_df_to_prophet\n",
    "from forecast_lib.utils.days_setting import minusndays\n",
    "from forecast_lib.utils.get_holidays_table import get_holidays_table\n",
    "from forecast_lib.utils.gen_regressors import gen_regressors\n",
    "from forecast_lib.utils.parameter_search import create_model_evaluation_udf\n",
    "from forecast_lib.utils.gen_params_indices import gen_params_indices\n",
    "from forecast_lib.utils.gen_weekly_aggregates_yoy import gen_weekly_aggregates_yoy\n",
    "from forecast_lib.utils.ts_cross_validation import ts_train_test_split, ts_train_test_backtest_split\n",
    "from forecast_lib.utils.evaluation_metrics import evaluation_smape\n",
    "from forecast_lib.utils.suppress_stdout_stderr import suppress_stdout_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Metrics = Metrics(start_date, end_date, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_values = Metrics.values.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gross_bookings'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_name = Metrics.metric_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_metric = metric_values.groupBy(\"yyyy_mm_dd\")\\\n",
    "                                    .agg(f.sum(metric_name).alias(metric_name)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perf = prepare_df_to_prophet(df = grouped_metric, metric_name = metric_name, tra = None, lamda = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap, floor = np.max(perf.y) * 1.1, np.min(perf.y) * 0.9\n",
    "perf[\"cap\"] = cap\n",
    "perf[\"floor\"]  = floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. get holiday tables\n",
    "holiday_tables = get_holidays_table(start_date = start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/blue-python/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py:1926: DeprecationWarning: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n",
      "  return a[slice1]-a[slice2]\n"
     ]
    }
   ],
   "source": [
    "rank_pos = metric_values\\\n",
    "        .where(\"yyyy_mm_dd >= '{cutoff_date}'\".format(cutoff_date =\n",
    "            (datetime.strptime(end_date, \"%Y-%m-%d\") - timedelta(365)).strftime(\"%Y-%m-%d\")))\\\n",
    "        .groupBy(\"pos\")\\\n",
    "        .agg(f.sum(metric_name).alias(metric_name))\\\n",
    "        .orderBy(metric_name, ascending=False).toPandas()\n",
    "rank_pos = rank_pos.assign(pos_rank = lambda x: x[metric_name].rank(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holiday_tables = holiday_tables.merge(rank_pos[['pos', 'pos_rank']])\n",
    "top5 = holiday_tables[holiday_tables['pos_rank']<=5][[ \"ds\", \"holiday\"]].drop_duplicates()\n",
    "other_pos_counts = holiday_tables[holiday_tables['pos_rank']>5].groupby(['ds', 'holiday']).count().reset_index()\n",
    "other_pos = other_pos_counts[other_pos_counts.pos >= 10][['ds', 'holiday']]\n",
    "filtered_holiday_table = top5.append(other_pos)\\\n",
    "                .drop_duplicates()\\\n",
    "                .assign(lower_window = 0, upper_window = 0)\n",
    "# 2.4 manually adjust for some holidays where the upper and lower window is bigger than 0\n",
    "# TODO: this has quite some subjectivity and require data inspection\n",
    "special_holidays = ['ChristmasEve', 'ThanksgivingDay','NewYearsDay']\n",
    "special_holidays_table = filtered_holiday_table[filtered_holiday_table['holiday']\\\n",
    "                                                .isin(special_holidays)]\\\n",
    "                                                .assign(lower_window = -1, upper_window = 1)\n",
    "special_holidays_table['upper_window'] = np.where(special_holidays_table['holiday'] ==  'ChristmasEve', 0,\n",
    "                                                   special_holidays_table['upper_window'])\n",
    "\n",
    "final_holiday_table = filtered_holiday_table[~filtered_holiday_table['holiday']\\\n",
    "                                     .isin(special_holidays)]\\\n",
    "                                     .append(special_holidays_table)\\\n",
    "                                     .sort_values(by = ['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_split = 14\n",
    "train = perf.iloc[:-train_test_split, ]\n",
    "test  = perf.iloc[-train_test_split:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 14:44:54,149 INFO       Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "2019-06-07 14:44:58,594 INFO       The daily smape for the default model is 2.15504998184\n"
     ]
    }
   ],
   "source": [
    "with suppress_stdout_stderr():\n",
    "    default_mod = Prophet(holidays = final_holiday_table, changepoint_range = 0.9)\n",
    "    forecast = default_mod.fit(train).predict(test)\n",
    "\n",
    "default_mape = evaluation_smape(eval_level = eval_level,\n",
    "                                forecast_horizon = forecast_horizon,\n",
    "                                true_df = test, forecast_df = forecast)\n",
    "logging.info(\"The {eval_level} smape for the default model is {default_mape}\".format(eval_level = eval_level,\n",
    "                                                                            default_mape = default_mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res = forecast[['ds', 'yhat']].merge(test[['ds', 'y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res ['residual'] = df_res ['yhat'] - df_res['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd.store_residual(df_input_prophet,df_forecast_tot,skill_name,model_used)\n",
    "df_forecast_tot2=fd.l1_module_residual(df_input_prophet,df_forecast_tot,pdf_residuals,skill_name)\n",
    "df_forecast_tot_adjsted2=fd.damp_trend_new(df_forecast_tot2,pdf_trend_dampening,skill_name,check=True)\n",
    "df_forecast_output=fd.extract_forecast_from_prophet(df_forecast_tot_adjsted2,skill_name,tra,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_res['residual2']=winsorize(df_res['residual'], limits=0.05, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>yhat</th>\n",
       "      <th>y</th>\n",
       "      <th>residual</th>\n",
       "      <th>residual2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-24</td>\n",
       "      <td>34445.131205</td>\n",
       "      <td>40152</td>\n",
       "      <td>-5706.868795</td>\n",
       "      <td>-5706.868795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>31402.297043</td>\n",
       "      <td>36464</td>\n",
       "      <td>-5061.702957</td>\n",
       "      <td>-5061.702957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-26</td>\n",
       "      <td>38006.798679</td>\n",
       "      <td>40688</td>\n",
       "      <td>-2681.201321</td>\n",
       "      <td>-2681.201321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>35956.946643</td>\n",
       "      <td>43905</td>\n",
       "      <td>-7948.053357</td>\n",
       "      <td>-7948.053357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-28</td>\n",
       "      <td>41170.784697</td>\n",
       "      <td>44632</td>\n",
       "      <td>-3461.215303</td>\n",
       "      <td>-3461.215303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>40441.664599</td>\n",
       "      <td>43181</td>\n",
       "      <td>-2739.335401</td>\n",
       "      <td>-2739.335401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>42948.371879</td>\n",
       "      <td>44961</td>\n",
       "      <td>-2012.628121</td>\n",
       "      <td>-2012.628121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>36398.257321</td>\n",
       "      <td>38543</td>\n",
       "      <td>-2144.742679</td>\n",
       "      <td>-2144.742679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>33287.775948</td>\n",
       "      <td>34713</td>\n",
       "      <td>-1425.224052</td>\n",
       "      <td>-1425.224052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-06-02</td>\n",
       "      <td>39816.325177</td>\n",
       "      <td>39683</td>\n",
       "      <td>133.325177</td>\n",
       "      <td>133.325177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>43156.597386</td>\n",
       "      <td>44813</td>\n",
       "      <td>-1656.402614</td>\n",
       "      <td>-1656.402614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>42820.319165</td>\n",
       "      <td>46407</td>\n",
       "      <td>-3586.680835</td>\n",
       "      <td>-3586.680835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>42015.321665</td>\n",
       "      <td>47364</td>\n",
       "      <td>-5348.678335</td>\n",
       "      <td>-5348.678335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>40671.508955</td>\n",
       "      <td>45190</td>\n",
       "      <td>-4518.491045</td>\n",
       "      <td>-4518.491045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds          yhat      y     residual    residual2\n",
       "0  2019-05-24  34445.131205  40152 -5706.868795 -5706.868795\n",
       "1  2019-05-25  31402.297043  36464 -5061.702957 -5061.702957\n",
       "2  2019-05-26  38006.798679  40688 -2681.201321 -2681.201321\n",
       "3  2019-05-27  35956.946643  43905 -7948.053357 -7948.053357\n",
       "4  2019-05-28  41170.784697  44632 -3461.215303 -3461.215303\n",
       "5  2019-05-29  40441.664599  43181 -2739.335401 -2739.335401\n",
       "6  2019-05-30  42948.371879  44961 -2012.628121 -2012.628121\n",
       "7  2019-05-31  36398.257321  38543 -2144.742679 -2144.742679\n",
       "8  2019-06-01  33287.775948  34713 -1425.224052 -1425.224052\n",
       "9  2019-06-02  39816.325177  39683   133.325177   133.325177\n",
       "10 2019-06-03  43156.597386  44813 -1656.402614 -1656.402614\n",
       "11 2019-06-04  42820.319165  46407 -3586.680835 -3586.680835\n",
       "12 2019-06-05  42015.321665  47364 -5348.678335 -5348.678335\n",
       "13 2019-06-06  40671.508955  45190 -4518.491045 -4518.491045"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with suppress_stdout_stderr():\n",
    "    final_mod = Prophet(holidays = final_holiday_table)\n",
    "    future = final_mod.fit(perf).make_future_dataframe(periods=forecast_horizon)\n",
    "    forecast = final_mod.predict(future).tail(forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# use the default prophet model with holidays as baseline\n",
    "# supress_stdout_stderr is to suppress the optimization output from Stan used by prophet package\n",
    "with suppress_stdout_stderr():\n",
    "    default_mod = Prophet(holidays = final_holiday_table, changepoint_range = 0.9)\n",
    "    forecast = default_mod.fit(train).predict(test)\n",
    "\n",
    "default_mape = evaluation_smape(eval_level = self.eval_level,\n",
    "                                forecast_horizon = self.forecast_horizon,\n",
    "                                true_df = test, forecast_df = forecast)\n",
    "logging.info(\"The {eval_level} smape for the default model is {default_mape}\".format(eval_level = self.eval_level,\n",
    "                                                                            default_mape = default_mape))\n",
    "# 5.1 run the model with Random search\n",
    "if self.optimizer == 'random_search':\n",
    "    logging.info(\"Cross Validation type is {cv_type}, start with random grid search for hyperparameters\".format(\n",
    "            cv_type = self.cv_type))\n",
    "    params_sp = gen_params_indices(df = train, pred_size = self.forecast_horizon, cv_type = self.cv_type)\n",
    "    # broadcast the in sample training set\n",
    "    bc_train = sc.broadcast(train)\n",
    "    # specify the udf\n",
    "    mod_eval_udf = create_model_evaluation_udf(eval_level = self.eval_level, perf_df = perf, bc_df = bc_train,\n",
    "                                               pred_size = self.forecast_horizon, holiday_table = final_holiday_table, \n",
    "                                               cap= cap, floor = floor, has_add_regressors = self.has_add_regressors)\n",
    "\n",
    "    results_sp = params_sp.select('params', f.expr('explode(train_indices)').alias('train_indices') )\\\n",
    "                          .withColumn('smape', mod_eval_udf(f.col(\"params\"), f.col(\"train_indices\")))\\\n",
    "                          .cache()\n",
    "\n",
    "    results_pd = results_sp.toPandas()\n",
    "    # get the meidan smape for each different combinations of hyperparameters and select the best median\n",
    "    avg_scores = results_pd.groupby('params', as_index = False)[['smape']].median()\n",
    "    best_params = avg_scores.sort_values('smape').iloc[0].params.asDict()\n",
    "    logging.info(\"Best hyperparameters found is {best_params}\".format(best_params = best_params))\n",
    "    # fit the model again using the best parameters with entire training set and evaluate the error\n",
    "    # based on out of sample test set.\n",
    "    with suppress_stdout_stderr():\n",
    "        best_mod = Prophet(daily_seasonality = False, interval_width = 0.95, holidays = final_holiday_table, **best_params)\n",
    "        forecast = best_mod.fit(train).predict(test)\n",
    "    rs_mape = evaluation_smape(eval_level = self.eval_level,\n",
    "                               forecast_horizon = self.forecast_horizon,\n",
    "                               true_df = test, forecast_df = forecast)\n",
    "    logging.info(\"The {eval_level} mape for the random grid search model is {rs_mape}\".format(eval_level = self.eval_level,\n",
    "                                                                                              rs_mape = rs_mape))\n",
    "# 5.2 compare the default baseline with random grid search and choose which model to use\n",
    "if self.optimizer == \"default\" or default_mape <= rs_mape:\n",
    "    logging.info(\"\"\"Default prophet configuration is the default or it is better than random search\n",
    "                therefore re-forecast using the entire dataset with default prophet\"\"\")\n",
    "    with suppress_stdout_stderr():\n",
    "        final_mod = Prophet(holidays = final_holiday_table)\n",
    "        future = final_mod.fit(perf).make_future_dataframe(periods=self.forecast_horizon)\n",
    "        forecast = final_mod.predict(future).tail(self.forecast_horizon)\n",
    "else:\n",
    "    logging.info(\"\"\"Random search is better than default prophet configuration\n",
    "                therefore re-forecast using the entire dataset with the best hyperparameters\"\"\")\n",
    "\n",
    "    # run the model with Random search or default, compare the default baseline with random grid search\n",
    "    with suppress_stdout_stderr():\n",
    "        final_mod = Prophet(daily_seasonality = False, interval_width = 0.95, holidays = final_holiday_table, **best_params)\n",
    "        future = final_mod.fit(perf).make_future_dataframe(periods=self.forecast_horizon)\n",
    "    if getattr(best_mod, 'growth') == 'logistic':\n",
    "        future['cap'] = cap\n",
    "        future['floor'] = floor\n",
    "    forecast = final_mod.predict(future)\n",
    "\n",
    "# 6. adjust for the recent performance AR\n",
    "\n",
    "# 7. produce the components plots and output data needed. \n",
    "components_fig = final_mod.plot_components(forecast)\n",
    "# get the predicted data\n",
    "prediction = forecast.tail(self.forecast_horizon)[['ds','yhat']].round({'yhat': 0})\n",
    "# generate the weekly data or weekly year-over-year data\n",
    "yoy = gen_weekly_aggregates_yoy(prediction = prediction, perf_last_year = perf,\n",
    "                                metric_name = self.metric_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perf = prepare_df_to_prophet(df = grouped_metric, metric_name = metric_name, tra = None, lamda = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_holidays_table(start_date, end_date):\n",
    "    \"\"\"function to get the most important holidays in the training and predicted dataset \n",
    "    it takes in for every account the booker countries and filter for the holidays of those countries\n",
    "    if a holiday is celebrated in more than 10 countries, then it be in the final holiday table  \n",
    "    \n",
    "    Args: \n",
    "        start_date: start date of the training dataset\n",
    "        end_date  : end date of the test dataset\n",
    "        christmas_new_year_dummy: True, when treating every day between Christmas and New year as a separate dummy variable\n",
    "    Returns:\n",
    "        spark dataframe with holidays and upper and lower windows \n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        yyyy_mm_dd AS ds, \n",
    "        name AS holiday,\n",
    "        upper(cc1) AS pos\n",
    "    FROM \n",
    "        csa.holidays_2000_2030_cc1 \n",
    "    WHERE \n",
    "        yyyy_mm_dd >= '{start_date}'\n",
    "    AND \n",
    "        yyyy_mm_dd <= '{end_date}'\n",
    "    AND \n",
    "        level = 1 \n",
    "\n",
    "    \"\"\".format(start_date = start_date, end_date = end_date)\n",
    "    holiday_table = spark.sql(query).toPandas()\n",
    "    \n",
    "    return holiday_table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 10:34:22,759 INFO       getting holidays\n"
     ]
    }
   ],
   "source": [
    "# set the cap and floor of the data in case there is\n",
    "cap, floor = np.max(perf.y) * 1.1, np.min(perf.y) * 0.9\n",
    "perf[\"cap\"] = cap\n",
    "perf[\"floor\"]  = floor\n",
    "\n",
    "# 2. get holiday tables\n",
    "\n",
    "logging.info(\"getting holidays\")\n",
    "holiday_tables = get_holidays_table(start_date = start_date, end_date = end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_pos = metric_values\\\n",
    "                .where(\"yyyy_mm_dd >= '{cutoff_date}'\".format(cutoff_date =\n",
    "                    (datetime.strptime(end_date, \"%Y-%m-%d\") - timedelta(365)).strftime(\"%Y-%m-%d\")))\\\n",
    "                .groupBy(\"pos\")\\\n",
    "                .agg(f.sum(metric_name).alias(metric_name))\\\n",
    "                .orderBy(metric_name, ascending=False).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 10:40:10,778 INFO       test the rank_pos\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"test the rank_pos\")\n",
    "rank_pos = rank_pos.assign(pos_rank = lambda x: x[metric_name].rank(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = pd.read_csv(\"tripadvisor_GrossBookings_daily_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_this_year = prediction.round({'yhat': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_this_year = pred_this_year.assign(ds = lambda x: pd.to_datetime(x['ds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_this_year['week'] = pred_this_year.ds.dt.week\n",
    "pred_this_year['year'] = pred_this_year.ds.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only keep those days where it is a full week for weekly aggregates\n",
    "pred_this_year = pred_this_year.groupby(['year', 'week']).filter(lambda x: x['yhat'].count() == 7)\n",
    "pred_this_year = pred_this_year.groupby(['year', 'week']).agg({\"yhat\": \"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Spark 2.3.1 (packages)",
   "language": "python",
   "name": "spark230withpackages"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
